{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Before you run your homework run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "\n",
    "assert os.path.exists('../homeworks/HW4.ipynb')\n",
    "\n",
    "info = sys.platform + '\\n' + sys.version + '\\n' + os.getcwd() + '\\n' + os.getlogin()\n",
    "name = hashlib.sha256(info.encode('utf-8')).hexdigest()\n",
    "with open('../other/hw-4-'+name,'w') as f:\n",
    "    f.write(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../other/hw-4-'+name,'r') as f:\n",
    "    info = f.read()\n",
    "name = hashlib.sha256(info.encode('utf-8')).hexdigest()\n",
    "assert os.path.exists('../other/hw-4-'+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Apply all of the supervised and unsupervised classification and clustering algorithms we learned so far for the [sonar dataset from UCI](http://archive.ics.uci.edu/ml/datasets/connectionist+bench+(sonar,+mines+vs.+rocks)).\n",
    "\n",
    "The code for downloading the data is below. Don't load the data again and again in each subtask, refer the data as `SONAR` after you run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with urlopen(\"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\") as fil:\n",
    "    SONAR = pd.read_csv(fil, delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the tasks into subtasks and into separate cells.  Also before your analysis for each subtask, write a short paragraph before explaining which algorithm you are going to use. Each subtask should look like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1: K-nn Classification\n",
    "\n",
    "For K-nn classification which is a supervised algorithm we need to divide the dataset into training and test. This is already a labeled dataset. The last column represents the type of solid whether it is rock or metal. So we have them as ys. I  preferred the \"euclidean\" type to divide the classes and decided to check the nearest 5 data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = SONAR.iloc[:,0:60]\n",
    "ys = SONAR.iloc[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(xs, ys, test_size = 0.25, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean').fit(Xtrain, Ytrain)\n",
    "predictions =  model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy score we see that the K-nn algorithm is not bad with classifying the SONAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20  1]\n",
      " [10 21]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7884615384615384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "cm = confusion_matrix(Ytest, predictions)\n",
    "print(cm)\n",
    "accuracy = accuracy_score(Ytest, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2: K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is an unsupervised algorithm to select clusters of a dataset. We basically have 3/4 of the data to train and the rest for test. Since there are 2 type(rock, metal) I take the number of clusters as 2 and the classification is done with the splitted X values. And \"R\" and \"M\" are not numerical values. That is why I converted them to 0 and 1 for accuracy score to function properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(xs, ys, test_size = 0.25, train_size = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "classifier = KMeans(n_clusters=2,random_state=1).fit(Xtest)\n",
    "predicted = classifier.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"R\":0, \"M\":1}\n",
    "real = Ytest.map(lambda x: labels[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score indicates that approximately half of the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36 75]\n",
      " [27 70]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5096153846153846"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(confusion_matrix(real,predicted))\n",
    "accuracy_score(real,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtask 3: Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting each datapoints as clusters by their distance to eachother the algorithm creates connected clusters. Thus we can see the form of a tree and gives us a better idea of how the clusters emerges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here is the code to plot a dendrogram by the smallest distances between points using the 'ward' method. I had to change M and R values to 1 and 2 because fcluster function automatically assignes 1 and 2 values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After prediction I changed the labels again to match the predicted ones. And accuracy score shows us the results which are not quite well. It is probable that this does not go well with SONAR dataset because there are many datapoints and since this method calculates the smallest distances to form clusters ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('SONAR Dendrogram')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('levels')\n",
    "dendrogram(linkage(xs, 'ward'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "predicted = fcluster(linkage(xs, 'ward'), 2, criterion='maxclust')\n",
    "\n",
    "labels1 = {\"M\":1, \"R\":2}\n",
    "real = ys.map(lambda x: labels1[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[36 75]\n",
      " [27 70]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5096153846153846"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(confusion_matrix(real,predicted))\n",
    "accuracy_score(real,predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 4: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have multiple columns and that means we have so many X values to predict Y which is either \"rock\" or \"mine\". So we have to reduce the number for a linear regression method. That is why we need to know the correlation between the columns. We use corr function with absolute values to have a correlation matrix. After selecting the upper triangle of the matrix we get rid of the columns which have more than 0.95 of correlation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign the new values as x. We still have lots of independent variables. y's are converted to 0 and 1 to have numerical data instead of strings. We fit the model and let it make predictions.In a new dataframe called 'df' we can see the real and predicted values together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary tells us that especially 49, 54 and 56 have a strong impact on the result while R^2 values is around 0.70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr_matrix = SONAR.corr().abs()\n",
    "\n",
    "from matplotlib import pyplot\n",
    "plt.imshow(corr_matrix, cmap='hot', interpolation='nearest')\n",
    "plt.show()\n",
    "\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "to_drop = [column for column in upper.columns if any(upper[column] < 0.05)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17,\n",
       " 18,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = to_drop\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = SONAR.iloc[:,selected_columns]\n",
    "y = SONAR.iloc[:,60:61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.replace(to_replace=['M', 'R'], value=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.757661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.180900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.579951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.465656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.748072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.769213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.467509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.489084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.689504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.616369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.872960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.170768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.430765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.121927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.287610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.654987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.911866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.544287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.482619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.628025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.028622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.311237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.715498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.470291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.279780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.368187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.118546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.002216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.089077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.546977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.513065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.678113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.507541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.624124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.155714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.511029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.371141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.404044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.598568</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     real  predicted\n",
       "0     1.0   0.757661\n",
       "1     1.0   1.180900\n",
       "2     1.0   0.579951\n",
       "3     1.0   0.465656\n",
       "4     1.0   0.748072\n",
       "5     1.0   0.944185\n",
       "6     1.0   0.769213\n",
       "7     1.0   0.391337\n",
       "8     1.0   0.467509\n",
       "9     1.0   0.489084\n",
       "10    1.0   0.689504\n",
       "11    1.0   0.616369\n",
       "12    1.0   0.872960\n",
       "13    1.0   1.170768\n",
       "14    1.0   0.430765\n",
       "15    1.0   1.121927\n",
       "16    1.0   1.287610\n",
       "17    1.0   0.654987\n",
       "18    1.0   0.911866\n",
       "19    1.0   0.544287\n",
       "20    1.0   0.482619\n",
       "21    1.0   0.628025\n",
       "22    1.0   0.324859\n",
       "23    1.0   0.496757\n",
       "24    1.0   1.028622\n",
       "25    1.0   0.644272\n",
       "26    1.0   0.311237\n",
       "27    1.0   0.560172\n",
       "28    1.0   0.909475\n",
       "29    1.0   0.715498\n",
       "..    ...        ...\n",
       "178   0.0   0.470291\n",
       "179   0.0   0.264894\n",
       "180   0.0  -0.279780\n",
       "181   0.0  -0.368187\n",
       "182   0.0  -0.006957\n",
       "183   0.0  -0.089279\n",
       "184   0.0  -0.118546\n",
       "185   0.0  -0.002216\n",
       "186   0.0  -0.089077\n",
       "187   0.0   0.050395\n",
       "188   0.0   0.204373\n",
       "189   0.0   0.546977\n",
       "190   0.0   0.513065\n",
       "191   0.0   0.678113\n",
       "192   0.0   0.507541\n",
       "193   0.0   0.624124\n",
       "194   0.0   0.435406\n",
       "195   0.0   0.176403\n",
       "196   0.0   0.041295\n",
       "197   0.0   0.162473\n",
       "198   0.0   0.155714\n",
       "199   0.0   0.033416\n",
       "200   0.0   0.015965\n",
       "201   0.0   0.142346\n",
       "202   0.0   0.511029\n",
       "203   0.0   0.022882\n",
       "204   0.0   0.371141\n",
       "205   0.0   0.584453\n",
       "206   0.0   0.404044\n",
       "207   0.0   0.598568\n",
       "\n",
       "[208 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(x,y)\n",
    "predictions = lm.predict(x)\n",
    "\n",
    "df = pd.DataFrame(np.c_[y,predictions],columns = [\"real\",\"predicted\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                     60   R-squared:                       0.694\n",
      "Model:                            OLS   Adj. R-squared:                  0.619\n",
      "Method:                 Least Squares   F-statistic:                     9.242\n",
      "Date:                Fri, 23 Nov 2018   Prob (F-statistic):           5.07e-26\n",
      "Time:                        23:14:33   Log-Likelihood:                -92.617\n",
      "No. Observations:                 208   AIC:                             267.2\n",
      "Df Residuals:                     167   BIC:                             404.1\n",
      "Df Model:                          41                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "17             0.1586      0.287      0.552      0.581      -0.408       0.726\n",
      "18             0.0486      0.313      0.155      0.877      -0.570       0.667\n",
      "21            -0.0834      0.308     -0.271      0.787      -0.691       0.524\n",
      "22            -0.1573      0.464     -0.339      0.735      -1.073       0.758\n",
      "23             0.1229      0.531      0.232      0.817      -0.925       1.171\n",
      "24            -0.4403      0.548     -0.804      0.423      -1.522       0.641\n",
      "25             1.0677      0.579      1.843      0.067      -0.076       2.212\n",
      "26            -0.5594      0.558     -1.002      0.318      -1.661       0.543\n",
      "27             0.1004      0.507      0.198      0.843      -0.901       1.102\n",
      "28             0.6472      0.459      1.410      0.160      -0.259       1.553\n",
      "29            -1.3552      0.483     -2.803      0.006      -2.310      -0.401\n",
      "30             1.9530      0.515      3.789      0.000       0.935       2.970\n",
      "31            -0.9868      0.510     -1.935      0.055      -1.993       0.020\n",
      "32             0.2957      0.509      0.581      0.562      -0.709       1.300\n",
      "33             0.0674      0.508      0.133      0.895      -0.936       1.071\n",
      "34            -0.4750      0.492     -0.966      0.335      -1.446       0.496\n",
      "35             1.0620      0.517      2.055      0.041       0.042       2.082\n",
      "36            -0.0021      0.500     -0.004      0.997      -0.989       0.985\n",
      "37             0.2675      0.530      0.505      0.614      -0.779       1.314\n",
      "38            -0.8246      0.525     -1.570      0.118      -1.862       0.212\n",
      "39             0.8645      0.511      1.691      0.093      -0.145       1.874\n",
      "40            -0.2489      0.532     -0.468      0.641      -1.300       0.802\n",
      "41             0.5327      0.559      0.953      0.342      -0.571       1.637\n",
      "42            -0.7490      0.646     -1.160      0.248      -2.024       0.526\n",
      "43            -0.1016      0.698     -0.146      0.884      -1.480       1.277\n",
      "44            -0.4403      0.775     -0.568      0.571      -1.971       1.090\n",
      "45            -0.4617      0.944     -0.489      0.625      -2.325       1.402\n",
      "46             0.3559      1.297      0.274      0.784      -2.204       2.916\n",
      "47            -0.9698      1.703     -0.569      0.570      -4.332       2.393\n",
      "48            -4.3474      2.449     -1.775      0.078      -9.182       0.487\n",
      "49            10.0796      3.912      2.577      0.011       2.356      17.803\n",
      "50             4.3267      4.042      1.070      0.286      -3.654      12.307\n",
      "51            -6.4173      4.848     -1.324      0.187     -15.989       3.155\n",
      "52            -2.5277      5.932     -0.426      0.671     -14.240       9.185\n",
      "53            -6.2978      5.356     -1.176      0.241     -16.872       4.276\n",
      "54            12.3777      5.870      2.109      0.036       0.788      23.967\n",
      "55             2.4746      7.753      0.319      0.750     -12.831      17.781\n",
      "56            10.5147      6.967      1.509      0.133      -3.239      24.269\n",
      "57            -8.1326      7.025     -1.158      0.249     -22.002       5.737\n",
      "58            -3.6195      8.148     -0.444      0.657     -19.706      12.467\n",
      "59             0.6491      8.298      0.078      0.938     -15.733      17.031\n",
      "==============================================================================\n",
      "Omnibus:                       17.640   Durbin-Watson:                   0.637\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):                6.641\n",
      "Skew:                           0.129   Prob(JB):                       0.0361\n",
      "Kurtosis:                       2.164   Cond. No.                         905.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "\n",
    "est = sm.OLS(y, x)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 5: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After assigning train and test datas again the labels are converted to 0 and 1. Logistic regression model splits the data points into half by a hyperplane. Fitting the model and making predictions, the accuracy score gives us a considerably high ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(xs, ys, test_size = 0.25, train_size = 0.75, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"R\":0, \"M\":1}\n",
    "Ytrain = Ytrain.map(lambda x: labels[x])\n",
    "\n",
    "labels = {\"R\":0, \"M\":1}\n",
    "Ytest = Ytest.map(lambda x: labels[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression()\n",
    "logisticRegr.fit(Xtrain, Ytrain)\n",
    "predictions = logisticRegr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15 11]\n",
      " [ 4 22]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7115384615384616"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(confusion_matrix(Ytest,predictions))\n",
    "accuracy_score(Ytest,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 6: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have only two labels as the type of solid we can apply Naive Bayes method to identify the classes. After splitting the data for training and test we use the function GaussianNB. Bayes' Theorem which gives the probability of A when B is given, plays and important role in this method. For each y result it looks for the common attributes and the probabilites between the data points. This way the algorithm decides which row values acts the same given the same x values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score indicates that maybe this is not the best method to choose the classes but it still gives a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(xs, ys, test_size = 0.25, train_size = 0.75) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(Xtrain,Ytrain)\n",
    "predictions = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 11]\n",
      " [ 2 19]]\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(Ytest,predictions)\n",
    "accuracy = accuracy_score(Ytest,predictions)\n",
    "print(cm)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
