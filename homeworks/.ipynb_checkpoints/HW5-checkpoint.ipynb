{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5\n",
    "\n",
    "For this homework, we are going to work with [*Indoor User Movement Prediction from RSS data*](https://archive.ics.uci.edu/ml/datasets/Indoor+User+Movement+Prediction+from+RSS+data) dataset from UCI.  The homework is due Friday, December 21st midnight. \n",
    "\n",
    "## Task 1\n",
    "\n",
    "Download the dataset and unzip it in under a subdirectory of `data` folder named `rss_data`.\n",
    "\n",
    "The files we are interested is in the subfolder `dataset`.  Each of these files whose names that start with `MovementAAL_RSS_` contain data collected from indivuduals. Each of these files represent a single data point.  There are 314 of these files, and hence, you have 314 data points.  Each file has 4 columns but the number of rows change from file to file.  \n",
    "\n",
    "There is also a file named `MovementALL_target.csv` in that folder. This file tells us the class each of these measurements are assigned. Some of these measurements are labelled with +1 and some are labelled with -1.\n",
    "\n",
    "## Task 2\n",
    "\n",
    "Construct a SVM model that separates +1 labelled data points from -1 data points.  You must first solve the problem that these datapoints do not have the same number of rows even though they all have the same number of columns. \n",
    "\n",
    "## Task 3\n",
    "\n",
    "Using [Keras](https://keras.io/getting-started/sequential-model-guide/) write a neural network model that separates +1 labelled data points from -1 data points.\n",
    "\n",
    "## Notes\n",
    "\n",
    "1. You must document each step of your tasks: what are you doing, why are you doing it, what problems you encountered and how you solved it.  All of these must be explained and documented.  Solutions without sufficient documentations will be penalized accordingly. 50% of your points will come from your code, while the other 50% will come from your explanations.\n",
    "\n",
    "1. You can use MS Excel to inspect the files, but loading them up to python using pandas and inspecting them there under jupyter is easier.\n",
    "\n",
    "3. Put the data in a separate subfolder of your `data` folder and rename it `rss_data`. I'll take points off if the data is not saved under the correct place.\n",
    "\n",
    "1. For both of Task 2 and Task 3, you must split your data into a train and test set, and then evaluate the accuracy of your model on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import neighbors\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import nan as Nan\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the SCV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = pd.read_csv('C:/Users/azad/Desktop/Data Analysis in Fund. Sciences/github.com/data/rss_data/MovementAAL_target.csv', \n",
    "                             names=('sequence_ID', 'class_label'), \n",
    "                             skiprows=(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(seq_id):\n",
    "    return pd.read_csv('C:/Users/azad/Desktop/Data Analysis in Fund. Sciences/github.com/data/rss_data/MovementAAL_RSS_%s.csv' % seq_id, \n",
    "                           names=('RSS_anchor1', 'RSS_anchor2','RSS_anchor3', 'RSS_anchor4'), \n",
    "                           skiprows=(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a function to create dataframes from each of the CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df):\n",
    "    frames = []\n",
    "    target = []\n",
    "    for idx, row in df.iterrows():\n",
    "        data = get_dataframe(row['sequence_ID'])\n",
    "        frames.append(data)\n",
    "        arr = [row['class_label']] * len(data)\n",
    "        target += arr\n",
    "        \n",
    "    return frames, target\n",
    "\n",
    "data_test, target_test = create_data(df_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sizes of input datas are different. Their number of rows are not the same. Let's find the maximum of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = 0\n",
    "\n",
    "for i in range(0,314):\n",
    "    if (len(X[i]) > 0):\n",
    "        max = len(X[i])\n",
    "max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I used a for loop to fill the rows with Nan values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_test\n",
    "y = target_test\n",
    "\n",
    "for i in range(0,314):\n",
    "    length = len(X[i])\n",
    "    length1 = 129 - length\n",
    "    for j in range(0,length1):\n",
    "        X[i] = X[i].append(pd.Series([np.nan]), ignore_index = True)\n",
    "    X[i] = X[i].iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of multiple rows and columns I intended to convert the datapoints into vectors. Thus I came up with the following idea: why not sum all the rows? Honestly I couldn't think of any other method. First I created an array with the size (314,4) with zeros because there are 314 datapoints and 4 rows in each of them. Then using nested for loops I got the sums of the rows and assigned those to the new array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = [None]*4\n",
    "xs = np.zeros((314,4))\n",
    "\n",
    "for j in range(0,314):\n",
    "    for i in range(0,4):\n",
    "        if(i==0):\n",
    "            total[0] = np.nansum(X[j].loc[:,'RSS_anchor1'])\n",
    "        elif(i==1):\n",
    "            total[1] = np.nansum(X[j].loc[:,'RSS_anchor2'])\n",
    "        elif(i==2):\n",
    "            total[2] = np.nansum(X[j].loc[:,'RSS_anchor3'])\n",
    "        else:\n",
    "            total[3] = np.nansum(X[j].iloc[:,3:4])\n",
    "    xs[j] = total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.142848,  10.24    , -12.90474 , -12.7     ])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also I droped the first row of the label values not to cause any problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target = df_target.drop('sequence_ID', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing kernel as linear and fitting the data, SVM takes place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azad\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[25, 16],\n",
       "       [ 4, 34]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys = df_target\n",
    "\n",
    "classifier = svm.SVC(kernel='linear')\n",
    "\n",
    "train_xs, test_xs, train_ys, test_ys = train_test_split(xs,ys,test_size=0.25)\n",
    "classifier.fit(train_xs,train_ys)\n",
    "\n",
    "predicted_ys = classifier.predict(test_xs)\n",
    "confusion_matrix(test_ys,predicted_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7468354430379747"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_ys,predicted_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not terrible. I ran the codes for a couple of times and got 60-70 percents approximately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task3: Neural Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from xmltodict import parse\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import hashlib\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Conv1D\n",
    "from keras.optimizers import Adam, Adamax, RMSprop, SGD\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some optimizers or loss functions only accept (0,1) in binary classification. So I had to convert (-1,1) to (1,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys.replace((1, -1), (1, 0), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into test and train again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xs, test_xs, train_ys, test_ys = train_test_split(xs,ys,test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I practiced multiple optimizers or models and this is the best result I got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "235/235 [==============================] - 1s 5ms/step - loss: 1.0590 - acc: 0.5149\n",
      "Epoch 2/10\n",
      "235/235 [==============================] - 0s 352us/step - loss: 0.6850 - acc: 0.5872\n",
      "Epoch 3/10\n",
      "235/235 [==============================] - 0s 373us/step - loss: 0.6484 - acc: 0.6298\n",
      "Epoch 4/10\n",
      "235/235 [==============================] - 0s 395us/step - loss: 0.6481 - acc: 0.6340\n",
      "Epoch 5/10\n",
      "235/235 [==============================] - 0s 369us/step - loss: 0.6360 - acc: 0.6426\n",
      "Epoch 6/10\n",
      "235/235 [==============================] - 0s 356us/step - loss: 0.6305 - acc: 0.6468\n",
      "Epoch 7/10\n",
      "235/235 [==============================] - 0s 382us/step - loss: 0.6279 - acc: 0.6426\n",
      "Epoch 8/10\n",
      "235/235 [==============================] - 0s 365us/step - loss: 0.6239 - acc: 0.6383\n",
      "Epoch 9/10\n",
      "235/235 [==============================] - 0s 380us/step - loss: 0.6208 - acc: 0.6596\n",
      "Epoch 10/10\n",
      "235/235 [==============================] - 0s 355us/step - loss: 0.6193 - acc: 0.6553\n",
      "79/79 [==============================] - 0s 6ms/step\n",
      "\n",
      "acc: 64.56%\n"
     ]
    }
   ],
   "source": [
    "input_dim = 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim = input_dim , activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'relu'))\n",
    "model.add(Dropout(0.01))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\n",
    "\n",
    "model.fit(train_xs, train_ys, epochs = 10, batch_size = 4)\n",
    "\n",
    "scores = model.evaluate(test_xs, test_ys)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After several trials accuracy didn't make me happy. There is probably more to improve in these codes. However this is the best I could do."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
